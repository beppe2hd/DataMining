{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Classification"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertebrate Dataset\n",
    "\n",
    "We use a variation of the vertebrate data described in Example 3.1 of Chapter 3. Each vertebrate is classified into one of 5 categories: mammals, reptiles, birds, fishes, and amphibians, based on a set of explanatory attributes (predictor variables). Except for \"name\", the rest of the attributes have been converted into a *one hot encoding* binary representation. To illustrate this, we will first load the data into a Pandas DataFrame object and display its content."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Data/vertebrate.csv',header='infer')\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limited number of training examples, suppose we convert the problem into a binary classification task (mammals versus non-mammals). We can do so by replacing the class labels of the instances to *non-mammals* except for those that belong to the *mammals* class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data['Class'] = data['Class'].replace(['fishes','birds','amphibians','reptiles'],'non-mammals')\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply Pandas cross-tabulation to examine the relationship between the Warm-blooded and Gives Birth attributes with respect to the class. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pd.crosstab([data['Warm-blooded'],data['Gives Birth']],data['Class'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that it is possible to distinguish mammals from non-mammals using these two attributes alone since each combination of their attribute values would yield only instances that belong to the same class. For example, mammals can be identified as warm-blooded vertebrates that give birth to their young. Such a relationship can also be derived using a decision tree classifier, as shown by the example given in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "In this section, we apply a decision tree classifier to the vertebrate dataset described in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "\n",
    "Y = data['Class']\n",
    "X = data.drop(['Name','Class'],axis=1)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini',max_depth=3)\n",
    "clf = clf.fit(X, Y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding commands will extract the predictor (X) and target class (Y) attributes from the vertebrate dataset and create a decision tree classifier object using entropy as its impurity measure for splitting criterion. The decision tree class in Python sklearn library also supports using 'gini' as impurity measure. The classifier above is also constrained to generate trees with a maximum depth equals to 3. Next, the classifier is trained on the labeled data using the fit() function. \n",
    "\n",
    "We can plot the resulting decision tree obtained after training the classifier. To do this, you must first install both graphviz (http://www.graphviz.org) and its Python interface called pydotplus (http://pydotplus.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, feature_names=X.columns, class_names=['mammals','non-mammals'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, suppose we apply the decision tree to classify the following test examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "testData = [['gila monster',0,0,0,0,1,1,'non-mammals'],\n",
    "           ['platypus',1,0,0,0,1,1,'mammals'],\n",
    "           ['owl',1,0,0,1,1,0,'non-mammals'],\n",
    "           ['dolphin',1,1,1,0,0,0,'mammals']]\n",
    "testData = pd.DataFrame(testData, columns=data.columns)\n",
    "testData"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the predictor and target class attributes from the test data and then apply the decision tree classifier to predict their classes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "testY = testData['Class']\n",
    "testX = testData.drop(['Name','Class'],axis=1)\n",
    "testX"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predY = clf.predict(testX)\n",
    "predY"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "predictions = pd.concat([testData['Name'],pd.Series(predY,name='Predicted Class')], axis=1)\n",
    "predictions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for platypus, which is an egg-laying mammal, the classifier correctly predicts the class label of the test examples. We can calculate the accuracy of the classifier on the test data as shown by the example given below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#print(f\"{testY.values} is type {type(testY)}\")\n",
    "#print(f\"{predY} is type {type(predY)}\")\n",
    "\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(testY, predY)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Iris Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None)\n",
    "data.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'class']\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = data.drop('class', axis=1).to_numpy()\n",
    "y = data['class'].to_numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "# Split the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"the first two train instance {X_train[0:2,:]}\")\n",
    "print(f\"the first two train class {y_train[0:2]}\")\n",
    "print(f\"the first two test instance {X_test[0:2,:]}\")\n",
    "print(f\"the first two test class {y_test[0:2]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## K-Nearest Neighbor Classifier"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 5\n",
    "clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "Y_predTrain = clf.predict(X_train)\n",
    "Y_predTest = clf.predict(X_test)\n",
    "\n",
    "trainAcc = accuracy_score(y_train, Y_predTrain)\n",
    "testAcc = accuracy_score(y_test, Y_predTest)\n",
    "\n",
    "print(f'Train Accuracy {trainAcc}')\n",
    "print(f'Test Accuracy {testAcc}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A more interesting dataset for KNN\n",
    "\n",
    "In this example we explore the influence of normalization. We will work with a dataset describing cells characteristic in order to chose if it is related to cancer or not. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Data/KNNAlgorithmDataset.csv')\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data['diagnosis'].unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = data.drop(['diagnosis', 'id'], axis=1).to_numpy()\n",
    "y = data['diagnosis'].to_numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X[0,:])\n",
    "print(X.shape)\n",
    "print(len(data.columns))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here the key step. Performing the StandardScaler() operation should improve the performace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X[0,:]) # comparing the new values with the old ones we can obser the difference",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n",
    "\n",
    "#print(f\"the first two train instance {X_train[0:2, :]}\")\n",
    "print(f\"the first two train class {y_train[0:2]}\")\n",
    "#print(f\"the first two test instance {X_test[0:2, :]}\")\n",
    "print(f\"the first two test class {y_test[0:2]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now apply a KNN classifier and compute train and test accuracy "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 5\n",
    "clf = KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "Y_predTrain = clf.predict(X_train)\n",
    "Y_predTest = clf.predict(X_test)\n",
    "\n",
    "trainAcc = accuracy_score(y_train, Y_predTrain)\n",
    "testAcc = accuracy_score(y_test, Y_predTest)\n",
    "\n",
    "print(f'Train Accuracy {trainAcc}')\n",
    "print(f'Test Accuracy {testAcc}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Overfitting\n",
    "\n",
    "To illustrate the problem of model overfitting, we consider a two-dimensional dataset containing 1500 labeled instances, each of which is assigned to one of two classes, 0 or 1. Instances from each class are generated as follows:\n",
    "1. Instances from class 1 are generated from a mixture of 3 Gaussian distributions, centered at [6,14], [10,6], and [14 14], respectively. \n",
    "2. Instances from class 0 are generated from a uniform distribution in a square region, whose sides have a length equals to 20.\n",
    "\n",
    "For simplicity, both classes have equal number of labeled instances. The code for generating and plotting the data is shown below. All instances from class 1 are shown in red while those from class 0 are shown in black."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "N = 1500\n",
    "\n",
    "mean1 = [6, 14]\n",
    "mean2 = [10, 6]\n",
    "mean3 = [14, 14]\n",
    "cov = [[3.5, 0], [0, 3.5]]  # diagonal covariance\n",
    "\n",
    "np.random.seed(50)\n",
    "X = np.random.multivariate_normal(mean1, cov, int(N/6))\n",
    "X = np.concatenate((X, np.random.multivariate_normal(mean2, cov, int(N/6))))\n",
    "X = np.concatenate((X, np.random.multivariate_normal(mean3, cov, int(N/6))))\n",
    "X = np.concatenate((X, 20*np.random.rand(int(N/2),2)))\n",
    "Y = np.concatenate((np.ones(int(N/2)),np.zeros(int(N/2))))\n",
    "\n",
    "plt.plot(X[:int(N/2),0],X[:int(N/2),1],'r+',X[int(N/2):,0],X[int(N/2):,1],'k.',ms=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we reserve 80% of the labeled data for training and the remaining 20% for testing. We then fit decision trees of different maximum depths (from 2 to 50) to the training set and plot their respective accuracies when applied to the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#########################################\n",
    "# Training and Test set creation\n",
    "#########################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.8, random_state=1)\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#########################################\n",
    "# Model fitting and evaluation\n",
    "#########################################\n",
    "\n",
    "maxdepths = [2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]\n",
    "\n",
    "trainAcc = np.zeros(len(maxdepths))\n",
    "testAcc = np.zeros(len(maxdepths))\n",
    "\n",
    "index = 0\n",
    "for depth in maxdepths:\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)\n",
    "    testAcc[index] = accuracy_score(Y_test, Y_predTest)\n",
    "    index += 1\n",
    "    \n",
    "#########################################\n",
    "# Plot of training and test accuracies\n",
    "#########################################\n",
    "    \n",
    "plt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that training accuracy will continue to improve as the maximum depth of the tree increases (i.e., as the model becomes more complex). However, the test accuracy initially improves up to a maximum depth of 5, before it gradually decreases due to model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bayes Classifier\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(X_train[0:5], y_train[0:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(f\"Number of mislabeled points out of a total {X_test.shape[0]} points : {(y_test != y_pred).sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "out = gnb.predict([[4.6, 3.2, 1.4, 0.2], [7.7, 3.8, 6.7, 2.2]])\n",
    "out_prob = gnb.predict_proba([[4.6, 3.2, 1.4, 0.2], [7.7, 3.8, 6.7, 2.2]])\n",
    "\n",
    "print(out)\n",
    "print(out_prob)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../Data/flight/train.csv\")\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "\n",
    "gender_set = set(df['Gender'])\n",
    "gender_Dic = {k:v for v, k in enumerate(gender_set)}\n",
    "df['Gender'].replace(gender_Dic, inplace=True)\n",
    "customer_type_set = set(df['Customer Type'])\n",
    "customer_type_Dic = {k:v for v, k in enumerate(customer_type_set)}\n",
    "df['Customer Type'].replace(customer_type_Dic, inplace=True)\n",
    "type_of_Travel_set = set(df['Type of Travel'])\n",
    "type_of_Travel_Dic = {k:v for v, k in enumerate(type_of_Travel_set)}\n",
    "df['Type of Travel'].replace(type_of_Travel_Dic, inplace=True)\n",
    "class_set = set(df['Class'])\n",
    "class_Dic = {k:v for v, k in enumerate(class_set)}\n",
    "df['Class'].replace(class_Dic, inplace=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "y = df['satisfaction']\n",
    "X = df.drop(['Unnamed: 0','id', 'satisfaction'],axis=1)\n",
    "\n",
    "clf = CategoricalNB(fit_prior=False)\n",
    "clf.fit(X, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(clf.predict(X[2:3]))\n",
    "print(clf.predict_proba(X[2:3]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../Data/flight/test.csv\")\n",
    "df['Gender'].replace(gender_Dic, inplace=True)\n",
    "df['Customer Type'].replace(customer_type_Dic, inplace=True)\n",
    "df['Type of Travel'].replace(type_of_Travel_Dic, inplace=True)\n",
    "df['Class'].replace(class_Dic, inplace=True)\n",
    "print(df.shape)\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y = df['satisfaction']\n",
    "X = df.drop(['Unnamed: 0','id', 'satisfaction'],axis=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = clf.predict(X)\n",
    "print(accuracy_score(pred, y))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pred!=y",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(clf.predict(X[5:6]))\n",
    "print(clf.predict_proba(X[5:6]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## SVM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1],[0,0]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_true, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "model = svm.SVC(kernel=\"linear\", C=C)\n",
    "#model = svm.SVC(kernel=\"rbf\", gamma=0.7, C=C)\n",
    "model.fit(X, labels_true)\n",
    "\n",
    "# title for the plots\n",
    "titles = \"SVC with linear kernel\",\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X,\n",
    "        response_method=\"predict\",\n",
    "        cmap=plt.cm.coolwarm,\n",
    "        alpha=0.8,\n",
    "        ax=ax,\n",
    "    )\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "ax.scatter(X0, X1, c=labels_true, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "plt.grid()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_true, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "SVMtrainAcc = []\n",
    "SVMtestAcc = []\n",
    "\n",
    "for param in C:\n",
    "\n",
    "    clf = SVC(C=param,kernel='linear')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    SVMtrainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    SVMtestAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "\n",
    "plt.plot(C, SVMtrainAcc, 'ro-', C, SVMtestAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that linear classifiers perform poorly on the data since the true decision boundaries between classes are nonlinear for the given 2-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Nonlinear Support Vector Machine\n",
    "\n",
    "The code below shows an example of using nonlinear support vector machine with a Gaussian radial basis function kernel to fit the 2-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]\n",
    "SVMtrainAcc = []\n",
    "SVMtestAcc = []\n",
    "\n",
    "for param in C:\n",
    "    clf = SVC(C=param,kernel='rbf',gamma='auto')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_predTrain = clf.predict(X_train)\n",
    "    Y_predTest = clf.predict(X_test)\n",
    "    SVMtrainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "    SVMtestAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "plt.plot(C, SVMtrainAcc, 'ro-', C, SVMtestAcc,'bv--')\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Accuracy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the nonlinear SVM can achieve a higher test accuracy compared to linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "An ensemble classifier constructs a set of base classifiers from the training data and performs classification by taking a vote on the predictions made by each base classifier. We consider 3 types of ensemble classifiers in this example: bagging, boosting, and random forest. Detailed explanation about these classifiers can be found in Section 4.10 of the book.\n",
    "\n",
    "In the example below, we fit 500 base classifiers to the 2-dimensional dataset using each ensemble method. The base classifier corresponds to a decision tree with maximum depth equals to 10."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "numBaseClassifiers = 500\n",
    "maxdepth = 10\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=numBaseClassifiers)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_predTrain = clf.predict(X_train)\n",
    "Y_predTest = clf.predict(X_test)\n",
    "trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "clf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=maxdepth),n_estimators=numBaseClassifiers)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_predTrain = clf.predict(X_train)\n",
    "Y_predTest = clf.predict(X_test)\n",
    "trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "clf = ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=maxdepth),n_estimators=numBaseClassifiers)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_predTrain = clf.predict(X_train)\n",
    "Y_predTest = clf.predict(X_test)\n",
    "trainAcc.append(accuracy_score(Y_train, Y_predTrain))\n",
    "testAcc.append(accuracy_score(Y_test, Y_predTest))\n",
    "\n",
    "methods = ['Random Forest', 'Bagging', 'AdaBoost']\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax1.bar([1.5,2.5,3.5], trainAcc)\n",
    "ax1.set_xticks([1.5,2.5,3.5])\n",
    "ax1.set_xticklabels(methods)\n",
    "ax2.bar([1.5,2.5,3.5], testAcc)\n",
    "ax2.set_xticks([1.5,2.5,3.5])\n",
    "ax2.set_xticklabels(methods)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Summary\n",
    "\n",
    "This section provides several examples of using Python sklearn library to build classification models from a given input data. We also illustrate the problem of model overfitting and show how to apply different classification methods to the given dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
